import torch
import joblib
import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
import os
import json
from tqdm import tqdm
from sklearn.preprocessing import StandardScaler

from config import config
from model import OilPriceTransformer
from data_loader import get_processed_data
from utils import get_device
from news_agent import NewsCrawler, DeepSeekAnalyzer 

def load_environment():
    device = get_device()
    
    # åŠ è½½ Scalers
    try:
        scaler_features = joblib.load("models/scaler_features.pkl")
        scaler_targets = joblib.load("models/scaler_targets.pkl")
        feature_names = joblib.load("models/feature_names.pkl")
    except:
        print("æœªæ‰¾åˆ° Scaler æ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œ train.pyã€‚")
        return None
    
    # åŠ è½½æ¨¡å‹
    model_path = f"models/{config.MODEL_PATH}"
    if os.path.exists(model_path):
        mod_time = os.path.getmtime(model_path)
        import datetime
        ts = datetime.datetime.fromtimestamp(mod_time).strftime('%Y-%m-%d %H:%M:%S')
        print(f"Loading Model: {model_path} (Last Modified: {ts})")
    else:
        print(f"ERROR: Model file {model_path} not found!")
        return None
        
    input_dim = len(feature_names)
    
    model = OilPriceTransformer(
        input_dim=input_dim, 
        hidden_dim=config.HIDDEN_DIM, 
        num_layers=config.NUM_LAYERS,
        dropout=config.DROPOUT,
        nhead=config.NHEAD if hasattr(config, 'NHEAD') else 4
    )
    try:
        # ä½¿ç”¨ weights_only=True åŠ è½½ä»¥æŠ‘åˆ¶è­¦å‘Šï¼Œå¦‚æœ torch ç‰ˆæœ¬å¤ªæ—§åˆ™å›é€€
        model.load_state_dict(torch.load(f"models/{config.MODEL_PATH}", map_location=device))
    except:
         model.load_state_dict(torch.load(f"models/{config.MODEL_PATH}", map_location=device, weights_only=False))
         
    model.to(device)
    model.eval()
    
    return model, scaler_features, scaler_targets, feature_names, device

def evaluate_and_plot_history(days_to_plot=200):
    """
    åœ¨è¿‘æœŸå†å²æ•°æ®ä¸Šå›æµ‹æ¨¡å‹å¹¶ç»˜åˆ¶è¯¦ç»†å›¾è¡¨
    """
    print(f"\næ­£åœ¨è¯„ä¼°è¿‡å» {days_to_plot} å¤©çš„æ¨¡å‹è¡¨ç°...")
    env = load_environment()
    if not env: return
    model, scaler_f, scaler_t, feature_names, device = env
    
    df = get_processed_data()
    # æˆ‘ä»¬éœ€è¦åºåˆ—ã€‚
    # è®©æˆ‘ä»¬é‡æ„è¿‡å» N å¤©çš„åºåˆ—ã€‚
    
    # æå–æ•°æ®
    data_feat = df[feature_names].values
    data_target = df[["Target_Price", "Oil_Close"]].values # Oil_Close æ˜¯å®é™…æ”¶ç›˜ä»·ã€‚Target_Price æ˜¯ä¸‹ä¸€å¤©çš„ç›®æ ‡ä»·æ ¼ã€‚

    # æˆ‘ä»¬æƒ³è¦ä½¿ç”¨ T-60..T æ¥é¢„æµ‹ T æ—¶åˆ»çš„ Target_Price

    predictions_price = []
    predictions_upper = []
    predictions_lower = []
    confidence_scores = []
    actual_prices = []
    dates = []

    # éå†æœ€å N å¤©
    # ç¡®ä¿æˆ‘ä»¬æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®ä½œä¸ºåºåˆ—é•¿åº¦
    start_idx = len(df) - days_to_plot
    if start_idx < config.SEQ_LENGTH:
        start_idx = config.SEQ_LENGTH
        
    indices = range(start_idx, len(df))
    
    print("æ­£åœ¨ç”Ÿæˆé¢„æµ‹...")
    with torch.no_grad():
        for i in tqdm(indices):
            # è¾“å…¥åºåˆ—: i-SEQ_LEN åˆ° i
            seq_raw = data_feat[i-config.SEQ_LENGTH : i]
            # ä¿®å¤è­¦å‘Š: åŒ…è£…åœ¨ DataFrame ä¸­
            seq_df = pd.DataFrame(seq_raw, columns=feature_names)
            seq_scaled = scaler_f.transform(seq_df)
            input_tensor = torch.FloatTensor(seq_scaled).unsqueeze(0).to(device)
            
            # é¢„æµ‹
            pred_price_scaled, pred_log_var, pred_vol_scaled, _ = model(input_tensor)
            
            # åå½’ä¸€åŒ– - è¿™é‡Œçš„ inv[0] æ˜¯é¢„æµ‹çš„å¯¹æ•°æ”¶ç›Šç‡ (Log Return)ï¼Œä¸æ˜¯ä»·æ ¼
            p_val = pred_price_scaled.cpu().numpy()[0][0]
            v_val = pred_vol_scaled.cpu().numpy()[0][0]
            
            # ä½¿ç”¨ scaler_targets è¿›è¡Œåå˜æ¢ (æ¢å¤åˆ°åŸå§‹é‡çº§)
            inv = scaler_t.inverse_transform([[p_val, v_val]])[0]
            pred_log_return = inv[0]
            pred_volatility = inv[1]
            
            # [æ ¸å¿ƒä¿®æ­£] ä»æ”¶ç›Šç‡è¿˜åŸä»·æ ¼
            # æ¨¡å‹ä½¿ç”¨çš„æ˜¯ç›´åˆ° i-1 çš„æ•°æ®åºåˆ—è¿›è¡Œé¢„æµ‹
            # åŸºå‡†ä»·æ ¼æ˜¯è¾“å…¥åºåˆ—æœ€åä¸€ä¸ªæ—¶é—´ç‚¹ (i-1) çš„æ”¶ç›˜ä»·
            last_close_price = df.iloc[i-1]['Oil_Close']
            
            # Price(T+1) = Price(T) * exp(Log_Return)
            final_price = last_close_price * np.exp(pred_log_return)
            
            # æ³¢åŠ¨ç‡ä¹Ÿæ˜¯ç›¸å¯¹çš„ï¼Œå¦‚æœéœ€è¦ç”»å›¾ï¼Œç›´æ¥ç”¨å³å¯
            final_vol = pred_volatility
            
            # ç½®ä¿¡åº¦
            log_var = pred_log_var.cpu().numpy()[0][0]
            sigma = np.sqrt(np.exp(log_var))
            conf = np.exp(-sigma) # ç®€åŒ–çš„ 0-1 åˆ†æ•°
            
            predictions_price.append(final_price)
            predictions_upper.append(final_price + final_vol/2)
            predictions_lower.append(final_price - final_vol/2)
            confidence_scores.append(conf)
            
            # çœŸå®ç›®æ ‡ (é¢„æµ‹å¯¹åº”çš„é‚£ä¸€å¤© i)
            # è¿™é‡Œçš„ i æ˜¯åºåˆ—ä¹‹åçš„ä¸€å¤©ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬è¦é¢„æµ‹çš„é‚£ä¸€å¤©
            # æ³¨æ„: df.iloc[i]['Target_Price'] æ˜¯ i+1 å¤©çš„ä»·æ ¼ï¼Œæˆ‘ä»¬é¢„æµ‹çš„æ˜¯ i
            if i < len(df):
                actual_prices.append(df.iloc[i]['Oil_Close'])
                dates.append(df.index[i])
            else:
                # è¶Šç•Œä¿æŠ¤
                pass
            
    # ç§»é™¤ NaN (å¦‚æœæœ‰) (æœ€åä¸€è¡Œå¯èƒ½åŒ…å« NaN target)
    valid_idx = [i for i, p in enumerate(actual_prices) if not np.isnan(p)]
    
    # è¿‡æ»¤åˆ—è¡¨
    dates = [dates[i] for i in valid_idx]
    actual = [actual_prices[i] for i in valid_idx]
    preds = [predictions_price[i] for i in valid_idx]
    upper = [predictions_upper[i] for i in valid_idx]
    lower = [predictions_lower[i] for i in valid_idx]
    confs = [confidence_scores[i] for i in valid_idx]
    
    # ç»˜å›¾
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True, gridspec_kw={'height_ratios': [3, 1]})
    
    # 1. ä»·æ ¼ & èŒƒå›´
    ax1.plot(dates, actual, label="Actual Oil Price (çœŸå®æ²¹ä»·)", color="black", linewidth=2)
    ax1.plot(dates, preds, label="AI Predicted Price (AIé¢„æµ‹æ²¹ä»·)", color="royalblue", linestyle="--")
    ax1.fill_between(dates, lower, upper, color="royalblue", alpha=0.2, label="Predicted Context (é¢„æµ‹ç½®ä¿¡åŒºé—´)")
    ax1.set_title("Oil Price Prediction vs Actual (Hybrid Transformer-LSTM)", fontsize=14)
    ax1.set_ylabel("Price (USD)")
    ax1.legend(loc="upper left")
    ax1.grid(True, alpha=0.3)
    
    # 2. ç½®ä¿¡åº¦
    ax2.plot(dates, confs, label="Model Confidence Score (æ¨¡å‹ç½®ä¿¡åº¦)", color="green")
    ax2.set_ylabel("Confidence (0-1)")
    ax2.set_xlabel("Date")
    ax2.fill_between(dates, 0, confs, color="green", alpha=0.1)
    ax2.set_ylim(0, 1.0)
    ax2.legend(loc="upper left")
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig("prediction_analysis.png", dpi=300)
    print("å›¾è¡¨å·²ä¿å­˜è‡³ prediction_analysis.png")

def explain_model_shap():
    print("\nå¼€å§‹ç‰¹å¾é‡è¦æ€§åˆ†æ...")
    env = load_environment()
    if not env: return
    model, scaler_f, scaler_t, feature_names, device = env
    
    # å‡†å¤‡æ•°æ®
    df = get_processed_data()
    data_feat = df[feature_names].values
    # ä¿®å¤è­¦å‘Š: data_feat æ˜¯ numpy æ•°ç»„ï¼Œå¦‚æœ scaler æ˜¯åœ¨ DataFrame ä¸Šæ‹Ÿåˆçš„ï¼Œtransform éœ€è¦ DataFrame
    # scaler_f.transform(data_feat) causing warning
    df_feat_temp = pd.DataFrame(data_feat, columns=feature_names)
    data_scaled = scaler_f.transform(df_feat_temp)
    
    # ä½¿ç”¨åŸºäºæ¢¯åº¦çš„æ˜¾è‘—æ€§ (è¾“å…¥æ¢¯åº¦) ä»£æ›¿ SHAP DeepExplainer
    # åŸå› : DeepExplainer åœ¨è¾ƒæ–°ç‰ˆæœ¬çš„ PyTorch ä¸­ LayerNorm/LSTM ä¸Šä¼šä¸­æ–­ã€‚
    # è¾“å…¥æ¢¯åº¦æ˜¯ç‰¹å¾é‡è¦æ€§çš„é²æ£’ä»£ç†ã€‚
    
    # å–æœ€è¿‘æ•°æ®çš„æ ·æœ¬ (ä¾‹å¦‚: æœ€å100å¤©)
    sample_size = 100
    if len(data_scaled) < config.SEQ_LENGTH + sample_size:
        print("æ•°æ®ä¸è¶³ä»¥è¿›è¡Œè§£é‡Šã€‚")
        return
        
    # åˆ›å»ºè¾“å…¥å¼ é‡æ‰¹æ¬¡
    inputs = []
    for i in range(len(data_scaled) - sample_size, len(data_scaled)):
        seq = data_scaled[i-config.SEQ_LENGTH : i]
        inputs.append(seq)
    
    input_tensor = torch.FloatTensor(np.array(inputs)).to(device)
    input_tensor.requires_grad = True
    
    # å‰å‘ä¼ æ’­
    pred_price, _, _, _ = model(input_tensor)
    
    # åå‘ä¼ æ’­ä»¥è·å–å…³äºè¾“å…¥çš„æ¢¯åº¦
    # é¢„æµ‹æ€»å’Œæ˜¯æ ‡é‡ï¼Œå…è®¸åå‘ä¼ æ’­
    pred_price.sum().backward()
    
    # æ¢¯åº¦: (Batch, Seq, Features)
    grads = input_tensor.grad.abs().cpu().numpy()
    
    # åœ¨ Batch å’Œ Sequence ä¸Šå–å¹³å‡ä»¥è·å¾—å…¨å±€ç‰¹å¾é‡è¦æ€§
    # æˆ‘ä»¬æƒ³çŸ¥é“å“ªä¸ª FEATURE æœ€é‡è¦ï¼Œæ— è®ºæ—¶é—´æ­¥é•¿å¦‚ä½•
    feature_importance = np.mean(grads, axis=(0, 1))
    
    # å½’ä¸€åŒ–åˆ° 0-1
    feature_importance = feature_importance / feature_importance.sum()
    
    # æ’åº
    sorted_idx = np.argsort(feature_importance)
    sorted_names = [feature_names[i] for i in sorted_idx]
    sorted_vals = feature_importance[sorted_idx]
    
    # ç»˜å›¾
    plt.figure(figsize=(10, 8))
    plt.barh(range(len(sorted_names)), sorted_vals, color='teal')
    plt.yticks(range(len(sorted_names)), sorted_names)
    plt.xlabel("Relative Importance Score (Gradient-based Impact)")
    plt.title("What drives Oil Prices? (AI Feature Analysis)")
    plt.tight_layout()
    plt.savefig("feature_importance.png", dpi=300)
    print("ç‰¹å¾é‡è¦æ€§å›¾è¡¨å·²ä¿å­˜è‡³ feature_importance.png")

def validate_model_performance():
    """
    åœ¨éƒ¨åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œä»¥æ£€æŸ¥è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆæƒ…å†µã€‚
    """
    print("\n--- å¼€å§‹è¯¦ç»†æ¨¡å‹éªŒè¯ ---")
    env = load_environment()
    if not env: return
    model, scaler_f, scaler_t, feature_names, device = env
    
    df = get_processed_data()
    
    # 1. æ£€æŸ¥æ•°æ®è´¨é‡
    print("\n[æ•°æ®è´¨é‡æ£€æŸ¥]")
    print(f"æ—¥æœŸèŒƒå›´: {df.index.min()} åˆ° {df.index.max()}")
    print(f"æ€»è¡Œæ•°: {len(df)}")
    print(f"ç¼ºå¤±å€¼: {df.isnull().sum().sum()}")
    print("æ ·ä¾‹æ•°æ® (å‰2è¡Œ):")
    print(df[feature_names].head(2))
    
    total_len = len(df)
    train_size = int((total_len - config.SEQ_LENGTH) * 0.8) + config.SEQ_LENGTH
    
    # å®šä¹‰è¯„ä¼°ç´¢å¼•
    # è®­ç»ƒè¯„ä¼°: å–è®­ç»ƒéƒ¨åˆ†çš„æœ€å300æ¡
    train_eval_start = max(config.SEQ_LENGTH, train_size - 300)
    train_indices = range(train_eval_start, train_size)
    
    # æµ‹è¯•è¯„ä¼°: ä» train_size åˆ°ç»“æŸ (å‡å»å¯é¢„æµ‹æ­¥æ•°)
    test_indices = range(train_size, total_len - config.PREDICT_STEPS)
    
    data_feat = df[feature_names].values
    
    def run_inference(indices, label):
        preds = []
        actuals = []
        dates = []
        uppers = []
        lowers = []
        
        print(f"æ­£åœ¨å¯¹ {label} é›†è¿è¡Œæ¨æ–­ ({len(indices)} æ ·æœ¬)...")
        with torch.no_grad():
            for i in tqdm(indices):
                if i < config.SEQ_LENGTH: continue
                
                # è¾“å…¥: [i-Seq ... i-1]
                seq_raw = data_feat[i-config.SEQ_LENGTH : i]
                # ä¿®å¤: åŒ…è£…åœ¨ DataFrame ä¸­ä»¥æ¶ˆé™¤è­¦å‘Š
                seq_df = pd.DataFrame(seq_raw, columns=feature_names)
                seq_scaled = scaler_f.transform(seq_df)
                
                input_tensor = torch.FloatTensor(seq_scaled).unsqueeze(0).to(device)
                

                pred_return_scaled, log_var, _, _ = model(input_tensor)
                
                # [å…³é”®ä¿®æ­£] åå½’ä¸€åŒ–é€»è¾‘é€‚é… "Log Return" ç›®æ ‡
                
                # 1. åå½’ä¸€åŒ–é¢„æµ‹å€¼ (å¾—åˆ°çœŸå®çš„ Log Return)
                pred_ret_val = pred_return_scaled.cpu().numpy()[0][0]
                # æ³¨æ„: æˆ‘ä»¬ç°åœ¨çš„ Target Scaler æ‹Ÿåˆçš„æ˜¯ [Log_Return, Volatility]
                # inverse_transform ä¼šè¿”å› [Log_Return_Real, Vol_Real]
                real_log_return = scaler_t.inverse_transform([[pred_ret_val, 0]])[0][0]
                
                # 2. è¿˜åŸä¸ºç»å¯¹ä»·æ ¼
                # Price(t) = Price(t-1) * exp(Log_Return)
                # è·å–å½“å¤©çš„æ”¶ç›˜ä»· (ä½œä¸ºåŸºå‡†) - ä¹Ÿå°±æ˜¯ input sequence çš„æœ€åä¸€ä¸ªç‚¹çš„æ”¶ç›˜ä»·
                # æ³¨æ„ seq_raw æ˜¯åŸå§‹ç‰¹å¾å€¼ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ° 'Oil_Close' æ‰€åœ¨çš„åˆ—
                if 'Oil_Close' in feature_names:
                     close_idx = list(feature_names).index('Oil_Close')
                     last_close_price = seq_raw[-1, close_idx]
                else:
                     # Fallback (ä¸åº”è¯¥å‘ç”Ÿ)
                     last_close_price = 1.0 
                     
                final_price = last_close_price * np.exp(real_log_return)
                
                # 3. å¤„ç†ä¸ç¡®å®šæ€§ (ç®€åŒ–å¤„ç†ï¼Œå‡è®¾ sigma æ˜¯é’ˆå¯¹ return çš„)
                start_log_var = log_var.cpu().numpy()[0][0]
                sigma_scaled = np.exp(0.5 * start_log_var)
                return_scale_factor = scaler_t.scale_[0]
                sigma_return = sigma_scaled * return_scale_factor
                
                # ä»·æ ¼åŒºé—´çš„è¿‘ä¼¼: Price * exp(Return +/- 1.96*Sigma)
                upper_price = last_close_price * np.exp(real_log_return + 1.96 * sigma_return)
                lower_price = last_close_price * np.exp(real_log_return - 1.96 * sigma_return)
                
                preds.append(final_price)
                uppers.append(upper_price)
                lowers.append(lower_price)
                
                # çœŸå®å€¼
                actual_val = df.iloc[i-1]['Target_Price']
                actuals.append(actual_val)
                dates.append(df.index[i-1])
                
        return dates, actuals, preds, uppers, lowers

    # è¿è¡Œ
    t_dates, t_act, t_pred, t_up, t_low = run_inference(train_indices, "TRAIN (è®­ç»ƒé›†å­é›†)")
    v_dates, v_act, v_pred, v_up, v_low = run_inference(test_indices, "TEST (æµ‹è¯•é›†)")
    
    # æŒ‡æ ‡è®¡ç®—
    def get_metrics(act, pred):
        act = np.array(act)
        pred = np.array(pred)
        if len(act) == 0: return 0, 0
        mse = np.mean((act - pred)**2)
        mae = np.mean(np.abs(act - pred))
        return mse, mae
        
    t_mse, t_mae = get_metrics(t_act, t_pred)
    v_mse, v_mae = get_metrics(v_act, v_pred)
    
    print(f"\n[æ€§èƒ½æŒ‡æ ‡]")
    print(f"è®­ç»ƒé›†å­é›† - MSE: {t_mse:.4f}, MAE: {t_mae:.4f}")
    print(f"æµ‹è¯•é›†     - MSE: {v_mse:.4f}, MAE: {v_mae:.4f}")
    
    # ç»˜å›¾
    try:
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))
        
        # è®­ç»ƒé›†ç»˜å›¾
        ax1.plot(t_dates, t_act, label="Actual (Target)", color='black')
        ax1.plot(t_dates, t_pred, label="Predicted", color='blue', linestyle='--')
        ax1.fill_between(t_dates, t_low, t_up, color='blue', alpha=0.15, label="95% CI")
        ax1.set_title(f"Training Set Fit (Last 300 days) - MAE: {t_mae:.2f}")
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æµ‹è¯•é›†ç»˜å›¾
        ax2.plot(v_dates, v_act, label="Actual (Target)", color='black')
        ax2.plot(v_dates, v_pred, label="Predicted", color='red', linestyle='--')
        ax2.fill_between(v_dates, v_low, v_up, color='red', alpha=0.15, label="95% CI")
        ax2.set_title(f"Test Set Evaluation (Unseen) - MAE: {v_mae:.2f}")
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig("model_validation_comparison.png", dpi=300)
        print("éªŒè¯å¯¹æ¯”å›¾å·²ä¿å­˜è‡³ model_validation_comparison.png")
    except Exception as e:
        print(f"ç»˜å›¾å¤±è´¥: {e}")

def predict_tomorrow(api_key=None):
    """
    ä½¿ç”¨å®æ—¶æ–°é—»åˆ†æé¢„æµ‹æ˜æ—¥æ²¹ä»·
    """
    print("\n=== å¼€å§‹å®æ—¶æ¨ç† (Live Inference) ===")
    
    # 1. åŠ è½½ç¯å¢ƒ
    env = load_environment()
    if not env: 
        print("ç¯å¢ƒåŠ è½½å¤±è´¥")
        return
    model, scaler_f, scaler_t, feature_names, device = env
    
    # 2. è·å–æœ€æ–°æ•°æ®åºåˆ—
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ‹¿åˆ°çš„ df åŒ…å«äº†ç›´åˆ°æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
    df = get_processed_data()
    
    # åªéœ€è¦æœ€å SEQ_LENGTH å¤©çš„æ•°æ®æ¥é¢„æµ‹æ˜å¤©
    if len(df) < config.SEQ_LENGTH:
        print("æ•°æ®ä¸è¶³ï¼")
        return

    last_sequence_df = df.iloc[-config.SEQ_LENGTH:].copy()
    
    # 3. è·å–å®æ—¶æ–°é—»æƒ…ç»ªåˆ†æ•° (æ›¿æ¢æ‰åŸæ¥çš„ VIX ä»£ç†åˆ†æ•°)
    print("\næ­£åœ¨è·å–ä»Šæ—¥å®æ—¶æ–°é—»...")
    print(f"DEBUG: API Key present: {bool(api_key)}")
    try:
        crawler = NewsCrawler()
        news_dict = crawler.fetch_investing_com_news()
        
        # å°†æŒ‰æ—¥æœŸåˆ†ç»„çš„æ–°é—»å­—å…¸å±•å¹³ä¸ºåˆ—è¡¨ï¼Œä¾› API åˆ†æ
        all_headlines = []
        for date_str, titles in news_dict.items():
            all_headlines.extend(titles)
        
        if all_headlines and api_key:
            print(f"è·å–åˆ° {len(all_headlines)} æ¡æ–°é—» (è¦†ç›– {len(news_dict)} å¤©)ï¼Œæ­£åœ¨è°ƒç”¨ DeepSeek è¿›è¡Œæƒ…æ„Ÿåˆ†æ...")
            analyzer = DeepSeekAnalyzer(api_key=api_key)
            ai_score = analyzer.analyze_sentiment(all_headlines)
            print(f"DeepSeek AI è¯„åˆ†ç»“æœ: {ai_score} (-1 æç©º ~ 1 æå¤š)")
            
            # å…³é”®æ­¥éª¤ï¼šä¿®æ”¹è¾“å…¥ç‰¹å¾ä¸­çš„ News_Impact
            # æˆ‘ä»¬åªä¿®æ”¹åºåˆ—ä¸­æœ€åä¸€å¤© (Today) çš„å› å­å€¼ï¼Œå‡è®¾æ–°é—»å½±å“æ˜¯å³æ—¶çš„
            if 'News_Impact' in feature_names:
                # å®šä½åˆ° News_Impact åˆ—
                last_sequence_df.iloc[-1, last_sequence_df.columns.get_loc('News_Impact')] = ai_score
                print("å·²åˆ©ç”¨ AI èˆ†æƒ…æŒ‡æ•°æ›´æ–°æ¨¡å‹è¾“å…¥")
            else:
                print("è­¦å‘Š: è®­ç»ƒç‰¹å¾ä¸­æœªæ‰¾åˆ° 'News_Impact'ï¼Œæ— æ³•æ³¨å…¥ AI å› å­")
        else:
            if not all_headlines:
                print("æœªæŠ“å–åˆ°ä»»ä½•æ–°é—»ã€‚")
            if not api_key:
                print("æœªæ£€æµ‹åˆ° API Key (api_key is None/Empty)ã€‚")
            print("å°†ä½¿ç”¨é»˜è®¤è®¡ç®—çš„ä»£ç†æŒ‡æ ‡ã€‚")
            default_score = last_sequence_df.iloc[-1]['News_Impact']
            print(f"é»˜è®¤ VIX ä»£ç†æŒ‡æ ‡å¾—åˆ†: {default_score}")

    except Exception as e:
        print(f"æ–°é—»æ¨¡å—å‡ºé”™ï¼Œå›é€€åˆ°é»˜è®¤æ•°æ®: {e}")

    # 4. é¢„å¤„ç† & æ¨ç†
    print("æ­£åœ¨è¿è¡Œç¥ç»ç½‘ç»œæ¨ç†...")
    try:
        # åªå–ç‰¹å¾åˆ—
        seq_feat = last_sequence_df[feature_names]
        
        # ç¼©æ”¾
        seq_scaled = scaler_f.transform(seq_feat)
        
        # è½¬æ¢ä¸º Tensor
        input_tensor = torch.FloatTensor(seq_scaled).unsqueeze(0).to(device)
        
        # æ¨¡å‹é¢„æµ‹
        with torch.no_grad():
            pred_return_scaled, log_var, _, _ = model(input_tensor)
            
            # [æ ¸å¿ƒä¿®æ­£] æ¨¡å‹é¢„æµ‹çš„æ˜¯å¯¹æ•°æ”¶ç›Šç‡ (Log Return)ï¼Œä¸æ˜¯ä»·æ ¼ï¼
            # éœ€è¦: 1. åå½’ä¸€åŒ–å¾—åˆ°çœŸå® Return  2. ç”¨æ˜¨æ”¶ * exp(Return) è¿˜åŸä»·æ ¼
            ret_val = pred_return_scaled.cpu().item()
            
            # åå½’ä¸€åŒ– Return (scaler_t æ‹Ÿåˆçš„æ˜¯ [Target_Return, Target_Volatility])
            real_return = scaler_t.inverse_transform([[ret_val, 0]])[0][0]
            
            # è·å–æ˜¨æ—¥æ”¶ç›˜ä»· (åºåˆ—æœ€åä¸€å¤©çš„ Oil_Close)
            if 'Oil_Close' in feature_names:
                last_close = last_sequence_df.iloc[-1]['Oil_Close']
            else:
                last_close = last_sequence_df.iloc[-1, 0]  # fallback
            
            # è¿˜åŸé¢„æµ‹ä»·æ ¼: P_tomorrow = P_today * exp(predicted_return)
            price = last_close * np.exp(real_return)
            
            # ä¸ç¡®å®šæ€§ (é’ˆå¯¹ Return çš„æ ‡å‡†å·®)
            sigma_scaled = np.exp(0.5 * log_var.cpu().item())
            ret_scale_factor = scaler_t.scale_[0]  # Return çš„ç¼©æ”¾å› å­
            sigma_ret = sigma_scaled * ret_scale_factor
            
            # ä»·æ ¼åŒºé—´ (åŸºäº Return çš„ç½®ä¿¡åŒºé—´è½¬æ¢ä¸ºä»·æ ¼)
            price_upper = last_close * np.exp(real_return + 1.96 * sigma_ret)
            price_lower = last_close * np.exp(real_return - 1.96 * sigma_ret)
            
            # ç½®ä¿¡åº¦
            conf_score = max(0.1, np.exp(-2.0 * abs(sigma_ret)))

        print("\n" + "="*50)
        print(f"  ğŸ›¢ï¸  é¢„æµ‹ç»“æœ (Prediction for Next Trading Day)")
        print("="*50)
        print(f"  æ˜¨æ—¥æ”¶ç›˜ä»·: ${last_close:.2f}")
        print(f"  é¢„æµ‹æ”¶ç›Šç‡: {real_return*100:.2f}%")
        print(f"  é¢„æµ‹ä»·æ ¼: ${price:.2f}")
        print(f"  ç½®ä¿¡åŒºé—´: [${price_lower:.2f}, ${price_upper:.2f}]")
        print(f"  æ¨¡å‹ç½®ä¿¡åº¦: {conf_score:.1%}")
        
        last_date = last_sequence_df.index[-1].strftime('%Y-%m-%d')
        print(f"  (åŸºäºæˆªæ­¢è‡³ {last_date} çš„æ•°æ®)")
        print("="*50 + "\n")
        
    except Exception as e:
        print(f"æ¨ç†è¿‡ç¨‹å‡ºé”™: {e}")

if __name__ == "__main__":
    # æ¨¡å¼é€‰æ‹©
    # 1. éªŒè¯æ¨¡å¼: å›æµ‹å†å²ï¼Œç”Ÿæˆå›¾è¡¨
    validate_model_performance() 
    
    # 2. ä¹Ÿæ˜¯éªŒè¯æ¨¡å¼: ç”Ÿæˆå®Œæ•´æµ‹è¯•é›†å›¾è¡¨
    print("\n--- ç”Ÿæˆå¸¦ç½®ä¿¡åŒºé—´çš„å®Œæ•´é¢„æµ‹å›¾ (AI å¢å¼ºç‰ˆ) ---")
    
    # è·å– API Key (è¯·ç¡®ä¿æ‚¨å·²è®¾ç½® DeepSeek_API ç¯å¢ƒå˜é‡ï¼Œæˆ–åœ¨æ­¤å¤„ç¡¬ç¼–ç )
    DEEPSEEK_API_KEY = os.getenv("DeepSeek_API") 
    # DEEPSEEK_API_KEY = "sk-xxxxxxxx" # æ‚¨çš„ Key
    
    # 1. é¢„å…ˆçˆ¬å–æ–°é—» (å¦‚æœæä¾›äº† Key)
    news_db = {}
    
    if DEEPSEEK_API_KEY:
        print("æ­£åœ¨æ£€æŸ¥å¹¶è¡¥å…¨è¿‡å»90å¤©çš„æ–°é—»æ•°æ® (DuckDuckGo Search)...")
        try:
            crawler = NewsCrawler()
            # æ™ºèƒ½è¡¥å…¨: è‡ªåŠ¨æ£€æŸ¥æœ¬åœ°æ˜¯å¦æœ‰ç¼ºå¤±çš„æ—¥æœŸå¹¶è”ç½‘æŠ“å–
            news_db = crawler.crawl_last_n_days(n=90)
            print(f"æ–°é—»åº“æœ€ç»ˆçŠ¶æ€: åŒ…å« {len(news_db)} å¤©çš„æ•°æ®")
            
        except Exception as e:
            print(f"çˆ¬è™«åˆå§‹åŒ–/è¿è¡Œå¤±è´¥: {e}")
            # é™çº§: å°è¯•è¯»å–æœ¬åœ°ç¼“å­˜
            if os.path.exists("crawled_news.json"):
                try:
                    with open("crawled_news.json", "r", encoding='utf-8') as f:
                        news_db = json.load(f)
                except: pass
    else:
        print("æœªæ£€æµ‹åˆ° API Keyï¼Œå°†è·³è¿‡åœ¨çº¿æ›´æ–°ï¼Œä»…å°è¯•è¯»å–æœ¬åœ°å†å²æ–°é—»...")
        if os.path.exists("crawled_news.json"):
            try:
                with open("crawled_news.json", "r", encoding='utf-8') as f:
                    news_db = json.load(f)
            except: pass

    env = load_environment()
    if env:
        model, scaler_f, scaler_t, features, device = env
        df = get_processed_data()
        
        # [æ ¸å¿ƒä¿®æ­£] ç»Ÿä¸€æ¨ç†æ¡ä»¶ï¼šæµ‹è¯•é›†å’Œ Full å›¾ä½¿ç”¨ç›¸åŒçš„æ•°æ®
        # ä¹‹å‰çš„é—®é¢˜ï¼šæµ‹è¯•é›†ç”¨ Oracle Newsï¼ŒFull å›¾ç”¨ VIX ä»£ç†ï¼Œå¯¼è‡´è¡¨ç°ä¸ä¸€è‡´
        # ç°åœ¨ï¼šä¸¤è€…éƒ½ä½¿ç”¨åŸå§‹ get_processed_data() çš„æ•°æ®ï¼ˆåŒ…å« Oracleï¼‰
        # è¿™æ ·å¯ä»¥å…¬å¹³å¯¹æ¯”ã€‚å¦‚æœç”¨æˆ·æœ‰çœŸæ­£çš„ DeepSeek æ–°é—»åˆ†æï¼ŒAI Model ä¼šç”¨é‚£ä¸ªã€‚
        
        # æ³¨æ„ï¼šè¿™é‡Œä¸å†é™çº§ News_Impactï¼Œä¿æŒåŸå§‹æ•°æ®
        # df["News_Impact"] = vix_proxy...  <- ç§»é™¤è¿™æ®µä»£ç 
             
        # é‡æ–°æå–ç‰¹å¾çŸ©é˜µ
        all_feat = df[features].values
        
        # è®¡ç®—æµ‹è¯•é›†èµ·ç‚¹ (ä¸ºäº†æ¼”ç¤ºæ•ˆæœï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨æœ€è¿‘ 90 å¤©çš„æ•°æ®)
        total_len = len(df)
        plot_days = 90 # æ‰©å¤§ä¸€ç‚¹èŒƒå›´
        # ç¡®ä¿ä¸è¶Šç•Œ
        start_idx = max(config.SEQ_LENGTH, total_len - plot_days)
        test_indices = range(start_idx, total_len)
        
        preds = []
        preds_ai = [] # å­˜å‚¨ AI å¢å¼ºåçš„é¢„æµ‹
        confidences = [] 
        uppers = []
        lowers = []
        actuals = []
        plot_dates = []
        
        # AI åˆ†æå™¨å®ä¾‹
        analyzer = None
        if DEEPSEEK_API_KEY:
            analyzer = DeepSeekAnalyzer(api_key=DEEPSEEK_API_KEY)
        
        print(f"å¼€å§‹æ¨ç†æœ€è¿‘ {len(test_indices)} å¤©çš„æ•°æ® (Base vs AI)...")
        
        # é¢„å…ˆæŸ¥æ‰¾ News_Impact åœ¨ç‰¹å¾ä¸­çš„åˆ—ç´¢å¼•
        news_feat_idx = -1
        if 'News_Impact' in features:
            news_feat_idx = list(features).index('News_Impact')

        with torch.no_grad():
            for i in tqdm(test_indices):
                # [æ ¸å¿ƒä¿®æ­£] é¢„æµ‹å¯¹é½é—®é¢˜
                # åºåˆ—: df[i-SEQ_LENGTH : i]  -> é¢„æµ‹ç›®æ ‡: df[i] çš„ä»·æ ¼
                # åºåˆ—æœ€åä¸€å¤©æ˜¯ df[i-1]ï¼Œæˆ‘ä»¬ç”¨å®ƒé¢„æµ‹ä¸‹ä¸€å¤© df[i]
                
                current_date = df.index[i-1]
                date_str = current_date.strftime('%Y-%m-%d')
                
                seq_raw = all_feat[i-config.SEQ_LENGTH : i].copy()
                
                # --- åˆ†æ”¯ A: æ ‡å‡†é¢„æµ‹ (ä½¿ç”¨å¼±åŒ–çš„ VIX ä»£ç†) ---
                seq_df = pd.DataFrame(seq_raw, columns=features)
                seq_scaled = scaler_f.transform(seq_df)
                input_tensor = torch.FloatTensor(seq_scaled).unsqueeze(0).to(device)
                
                # Model è¾“å‡º Return
                pred_ret, log_var, _, _ = model(input_tensor)
                
                # 1. è¿˜åŸ Price (Base)
                p_ret_val = pred_ret.cpu().item()
                real_ret = scaler_t.inverse_transform([[p_ret_val, 0]])[0][0]
                
                # è·å–æ˜¨æ”¶ (åºåˆ—æœ€åä¸€å¤©ï¼Œå³ df[i-1])
                if 'Oil_Close' in features:
                     last_close_price = seq_raw[-1, list(features).index('Oil_Close')]
                else: 
                     last_close_price = df.iloc[i-1]['Oil_Close']
                
                # é¢„æµ‹ä»Šå¤© (df[i]) çš„ä»·æ ¼
                price = last_close_price * np.exp(real_ret)
                
                # ä¸ç¡®å®šæ€§ (é’ˆå¯¹ Return)
                sigma_scaled = np.exp(0.5 * log_var.cpu().item())
                ret_scale_factor = scaler_t.scale_[0] 
                sigma_ret = sigma_scaled * ret_scale_factor
                
                preds.append(price)
                # ä»·æ ¼åŒºé—´
                uppers.append(last_close_price * np.exp(real_ret + 1.96 * sigma_ret))
                lowers.append(last_close_price * np.exp(real_ret - 1.96 * sigma_ret))
                
                conf_score = np.exp(-0.5 * sigma_ret) # ç®€åŒ–
                confidences.append(conf_score) 
                
                # --- åˆ†æ”¯ B: AI å¢å¼ºé¢„æµ‹ (æ³¨å…¥çœŸå®å†å²æ–°é—») ---
                price_ai = price # é»˜è®¤
                
                # åªæœ‰åœ¨æœ‰æ–°é—»ä¸”æ‰¾åˆ°äº†ç‰¹å¾åˆ—æ—¶æ‰è¿›è¡Œå¢å¼º
                if news_feat_idx >= 0 and date_str in news_db:
                    # è·å–è¯¥æ—¥æ–°é—»
                    daily_news = news_db[date_str]
                    
                    if analyzer:
                         # ç¼“å­˜é€»è¾‘
                         if not hasattr(analyzer, 'cache'): analyzer.cache = {}
                         if date_str in analyzer.cache:
                             ai_score = analyzer.cache[date_str]
                         else:
                             if len(daily_news) > 0:
                                 # ç®€å•é™æµ: å¦‚æœæ˜¯ DuckDuckGo å¾—åˆ°çš„ç©ºæ–°é—»ï¼Œä¸è°ƒç”¨
                                 ai_score = analyzer.analyze_sentiment(daily_news)
                             else:
                                 ai_score = 0
                             analyzer.cache[date_str] = ai_score
                    else:
                        ai_score = 0
                    
                    # æ„é€ æ–°çš„åºåˆ—ç”¨äº AI æ¨ç†
                    seq_ai = seq_raw.copy()
                    seq_ai[-1, news_feat_idx] = ai_score 
                    
                    # é‡æ–°ç¼©æ”¾ & æ¨ç†
                    seq_ai_df = pd.DataFrame(seq_ai, columns=features)
                    seq_ai_scaled = scaler_f.transform(seq_ai_df)
                    input_tensor_ai = torch.FloatTensor(seq_ai_scaled).unsqueeze(0).to(device)
                    
                    pred_ret_ai, _, _, _ = model(input_tensor_ai)
                    
                    p_ret_val_ai = pred_ret_ai.cpu().item()
                    real_ret_ai = scaler_t.inverse_transform([[p_ret_val_ai, 0]])[0][0]
                    
                    # è¿˜åŸä»·æ ¼
                    price_ai = last_close_price * np.exp(real_ret_ai)
                
                preds_ai.append(price_ai)
                
                # [æ ¸å¿ƒä¿®æ­£] çœŸå®å€¼å¯¹é½
                # æˆ‘ä»¬é¢„æµ‹çš„æ˜¯ df[i] é‚£å¤©çš„ä»·æ ¼ï¼Œæ‰€ä»¥çœŸå®å€¼å°±æ˜¯ df.iloc[i]['Oil_Close']
                try:
                    actual_price = df.iloc[i]['Oil_Close']
                    actuals.append(actual_price)
                    plot_dates.append(df.index[i])  # æ—¥æœŸä¹Ÿåº”è¯¥æ˜¯é¢„æµ‹ç›®æ ‡æ—¥ df[i]
                except:
                    pass

        # ç»˜å›¾ 
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=True, gridspec_kw={'height_ratios': [3, 1]})
        
        # [è°ƒè¯•] è¾“å‡ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
        print(f"\n[è°ƒè¯•ä¿¡æ¯]")
        print(f"é¢„æµ‹æ•°æ®ç‚¹æ•°: {len(preds)}")
        print(f"çœŸå®æ•°æ®ç‚¹æ•°: {len(actuals)}")
        print(f"é¢„æµ‹ä»·æ ¼èŒƒå›´: ${min(preds):.2f} - ${max(preds):.2f}")
        print(f"çœŸå®ä»·æ ¼èŒƒå›´: ${min(actuals):.2f} - ${max(actuals):.2f}")
        print(f"å¹³å‡é¢„æµ‹è¯¯å·®: ${np.mean(np.abs(np.array(preds) - np.array(actuals))):.2f}")
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        corr = np.corrcoef(preds[:len(actuals)], actuals)[0, 1]
        print(f"é¢„æµ‹ä¸çœŸå®çš„ç›¸å…³ç³»æ•°: {corr:.3f}")
        
        # é¡¶éƒ¨: ä»·æ ¼å¯¹æ¯”
        # [ä¿®æ”¹] ä¼˜åŒ–ç»˜å›¾æ ·å¼ä»¥è§£å†³é®æŒ¡é—®é¢˜
        # Base Model: ç°è‰²ç²—å®çº¿ï¼ŒåŠé€æ˜èƒŒæ™¯
        ax1.plot(plot_dates, preds, label="Base Model (VIX Proxy)", color="gray", linewidth=4, alpha=0.4)
        
        # AI Model: è“è‰²ç»†çº¿+ç‚¹çŠ¶ï¼Œå åŠ åœ¨ä¸Šå±‚
        # ä»…å½“ AI é¢„æµ‹ä¸æ™®é€šé¢„æµ‹ä¸åŒæ—¶æ‰ä¼šæœ‰æ˜æ˜¾çš„è§†è§‰å·®å¼‚
        ax1.plot(plot_dates, preds_ai, label="AI-Enhanced Prediction (Real News)", color="royalblue", linewidth=1.5, linestyle="-.")
        
        ax1.fill_between(plot_dates, lowers, uppers, color="royalblue", alpha=0.15, label="95% Confidence Interval")
        ax1.set_title(f"Oil Price Prediction: AI News vs VIX Proxy (Last {len(plot_dates)} Days)")
        ax1.set_ylabel("Price (USD)")
        
        # å¼ºåˆ¶æŠŠçœŸå®ä»·æ ¼ç”»åœ¨æœ€æœ€ä¸Šå±‚ï¼Œé»‘è‰²ç»†å®çº¿
        ax1.plot(plot_dates, actuals, label="Actual Price", color="black", linewidth=1, alpha=0.9, zorder=10)
        
        # [è¯Šæ–­ç»˜å›¾] ç»˜åˆ¶ Shift(-1) çš„çœŸå®ä»·æ ¼æ›²çº¿ (Yesterday's Price)
        # å¦‚æœé¢„æµ‹çº¿ä¸è¿™æ¡çº¿é‡åˆï¼Œè¯´æ˜æ¨¡å‹é€€åŒ–ä¸º Trivial Identity (Persistence Model)
        # ç”¨è™šçº¿ç»˜åˆ¶
        shifted_actuals = [0] + actuals[:-1]
        if len(shifted_actuals) == len(plot_dates):
             ax1.plot(plot_dates, shifted_actuals, label="Persistence Baseline (T-1)", color="gray", linewidth=1, linestyle=":", alpha=0.5)
        
        ax1.legend(loc="upper left")
        ax1.grid(True, alpha=0.3)
        
        # åº•éƒ¨: ç½®ä¿¡åº¦
        ax2.plot(plot_dates, confidences, label="Model Confidence Score", color="green", linewidth=1.5)
        ax2.fill_between(plot_dates, 0, confidences, color="green", alpha=0.1)
        ax2.set_ylabel("Confidence")
        ax2.set_xlabel("Date")
        ax2.set_ylim(0, 1.05)
        ax2.legend(loc="upper left")
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # [ä¿®æ­£] æ·»åŠ æ—¶é—´æˆ³å’Œæ›´å¤šå…ƒä¿¡æ¯åˆ°å›¾è¡¨
        import datetime
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        fig.text(0.99, 0.01, f'Generated: {timestamp} | MAE: ${np.mean(np.abs(np.array(preds) - np.array(actuals))):.2f} | Corr: {corr:.3f}', 
                ha='right', va='bottom', fontsize=8, alpha=0.7)
        
        plt.savefig("oil_price_prediction_full.png", dpi=300)
        print(f"\nâœ… å¢å¼ºç‰ˆé¢„æµ‹å›¾å·²ä¿å­˜è‡³ oil_price_prediction_full.png (ç”Ÿæˆæ—¶é—´: {timestamp})")

    explain_model_shap()          

    # Real-time inference
    predict_tomorrow(api_key=DEEPSEEK_API_KEY)  

